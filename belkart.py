# belkart.py
import os
import time
import json
import re
from collections import defaultdict
from typing import List, Dict, Any, Optional, Callable, Tuple

import requests
from bs4 import BeautifulSoup
from urllib.parse import urljoin

from dotenv import load_dotenv
from back_db import save_partners

from gigachat import GigaChat

ProgressFn = Optional[Callable[[int, int, str], None]]

BASE_URL = "https://belkart.by/BELKART/reklamnye-aktsii/"

load_dotenv()
GIGACHAT_TOKEN = os.getenv("GIGACHAT_TOKEN")

gc = GigaChat(
    credentials=GIGACHAT_TOKEN,
    scope="GIGACHAT_API_B2B",
    verify_ssl_certs=False,
    model="GigaChat-2-Max",
)
print("GIGACHAT_TOKEN =", GIGACHAT_TOKEN)

# –∫—ç—à –¥–ª—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ GigaChat: "raw_title raw_status" -> {"company": ..., "bonus": ...}
_GIGA_CACHE: Dict[str, Dict[str, Any]] = {}


# ---------- –í–°–ü–û–ú–û–ì–ê–¢–ï–õ–¨–ù–´–ï –§–£–ù–ö–¶–ò–ò ----------

def nlp_company_bonus(raw_text: str) -> Dict[str, Any]:
    """
    –û–±—ë—Ä—Ç–∫–∞ –Ω–∞–¥ extract_company_and_bonus —Å –ø—Ä–æ—Å—Ç—ã–º in‚Äëmemory –∫—ç—à–µ–º.
    –û–¥–∏–Ω –∏ —Ç–æ—Ç –∂–µ —Ç–µ–∫—Å—Ç –Ω–µ –æ—Ç–ø—Ä–∞–≤–ª—è–µ–º –≤ GigaChat –ø–æ–≤—Ç–æ—Ä–Ω–æ.
    """
    raw_text = raw_text.strip()
    if not raw_text:
        return {"company": None, "bonus": None}

    if raw_text in _GIGA_CACHE:
        return _GIGA_CACHE[raw_text]

    data = extract_company_and_bonus(raw_text)
    _GIGA_CACHE[raw_text] = data
    return data


def normalize_bonus(bonus) -> Optional[str]:
    """
    –ü—Ä–∏–≤–æ–¥–∏—Ç –±–æ–Ω—É—Å –∫ –∞–∫–∫—É—Ä–∞—Ç–Ω–æ–π —Å—Ç—Ä–æ–∫–µ:
    - –µ—Å–ª–∏ —Å–ø–∏—Å–æ–∫ ‚Üí —Å–∫–ª–µ–∏–≤–∞–µ–º;
    - —É–±–∏—Ä–∞–µ–º –ª–∏—à–Ω–∏–µ –ø—Ä–æ–±–µ–ª—ã –∏ –ø–µ—Ä–µ–≤–æ–¥—ã —Å—Ç—Ä–æ–∫;
    - –ø—É—Å—Ç–æ–µ ‚Üí None.
    """
    if bonus is None:
        return None

    if isinstance(bonus, list):
        bonus = " ".join(str(b).strip() for b in bonus if b)

    bonus = str(bonus).strip()
    bonus = " ".join(bonus.split())
    return bonus or None


def extract_bonus_number(bonus_str: Optional[str]) -> float:
    """
    –ò–∑–≤–ª–µ–∫–∞–µ—Ç —á–∏—Å–ª–æ–≤–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ –∏–∑ —Å—Ç—Ä–æ–∫–∏ –±–æ–Ω—É—Å–∞.
    –ü—Ä–∏–º–µ—Ä—ã: "—Å–∫–∏–¥–∫–∞ 15%" ‚Üí 15, "20 –¥–Ω–µ–π" ‚Üí 20, "2 –∫–Ω–∏–≥–∏ –≤ –ø–æ–¥–∞—Ä–æ–∫" ‚Üí 2.
    """
    if not bonus_str:
        return -1.0

    numbers = re.findall(r"\d+(?:[\.,]\d+)?", str(bonus_str))
    if not numbers:
        return -1.0

    try:
        return float(numbers[0].replace(",", "."))
    except Exception:
        return -1.0


# ---------- GIGACHAT ----------

def extract_company_and_bonus(text: str) -> dict:
    """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –∫–æ–º–ø–∞–Ω–∏—é –∏ –±–æ–Ω—É—Å —á–µ—Ä–µ–∑ GigaChat."""
    text = text.strip()
    if not text:
        return {"company": None, "bonus": None}

    prompt = f"""
–ò–∑–≤–ª–µ–∫–∏ –∏–∑ —Ç–µ–∫—Å—Ç–∞ –Ω–∞–∑–≤–∞–Ω–∏–µ –∫–æ–º–ø–∞–Ω–∏–∏ –∏ —Ä–∞–∑–º–µ—Ä –±–æ–Ω—É—Å–∞.
–ï—Å–ª–∏ –±–æ–Ω—É—Å —É–∫–∞–∑–∞–Ω –≤ —Ä–∞–∑–Ω—ã—Ö —Ñ–æ—Ä–º–∞—Ç–∞—Ö (—Å–∫–∏–¥–∫–∞ 15%, 15%, -15%, –∫–µ—à–±—ç–∫ 20%, 2 –∫–Ω–∏–≥–∏ –≤ –ø–æ–¥–∞—Ä–æ–∫), –≤–µ—Ä–Ω–∏ –µ–≥–æ –∫–∞–∫ –µ—Å—Ç—å.

–¢–µ–∫—Å—Ç:
\"\"\"{text}\"\"\"

–í–µ—Ä–Ω–∏ –°–¢–†–û–ì–û JSON –±–µ–∑ –ø–æ—è—Å–Ω–µ–Ω–∏–π:
{{
  "company": "...",
  "bonus": "..."
}}

–ï—Å–ª–∏ –¥–∞–Ω–Ω—ã—Ö –Ω–µ—Ç ‚Äì –∏—Å–ø–æ–ª—å–∑—É–π null.
"""

    try:
        resp = gc.chat(prompt)
        raw = resp.choices[0].message.content.strip()
        raw = raw.replace("```json", "").replace("```", "").strip()
        data = json.loads(raw)

        company = data.get("company")
        bonus = normalize_bonus(data.get("bonus"))

        return {"company": company, "bonus": bonus}
    except Exception as e:
        print(f"‚ö†Ô∏è GigaChat error: {e}")
        return {"company": None, "bonus": None}


# ---------- –ü–ê–†–°–ò–ù–ì –°–¢–†–ê–ù–ò–¶ ----------

def _parse_page(url: str, retry_count: int = 3) -> Tuple[List[Dict[str, Any]], BeautifulSoup]:
    """
    –ü–∞—Ä—Å–∏—Ç –æ–¥–Ω—É —Å—Ç—Ä–∞–Ω–∏—Ü—É —Å –ø–æ–≤—Ç–æ—Ä–Ω—ã–º–∏ –ø–æ–ø—ã—Ç–∫–∞–º–∏ –ø—Ä–∏ —Å–µ—Ç–µ–≤—ã—Ö –æ—à–∏–±–∫–∞—Ö.
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ø–∏—Å–æ–∫ –ø–∞—Ä—Ç–Ω—ë—Ä–æ–≤ –∏ –æ–±—ä–µ–∫—Ç BeautifulSoup.
    """
    last_error: Optional[str] = None

    for attempt in range(1, retry_count + 1):
        try:
            print(f"  üì° –ü–æ–ø—ã—Ç–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ {attempt}/{retry_count}: {url}")
            resp = requests.get(url, timeout=20)
            resp.raise_for_status()

            soup = BeautifulSoup(resp.text, "lxml")
            cards = soup.select("ul.card-list li.card-list__item")
            print(f"    üîç –ù–∞–π–¥–µ–Ω–æ –∫–∞—Ä—Ç–æ—á–µ–∫: {len(cards)}")

            results: List[Dict[str, Any]] = []

            for i, card in enumerate(cards, start=1):
                try:
                    link_tag = card.select_one("a.card-list__link")
                    title_tag = card.select_one(".card-list__title")
                    status_tag = card.select_one(".card-list__label")

                    raw_title = (title_tag.text or "").strip() if title_tag else ""
                    raw_status = (status_tag.text or "").strip() if status_tag else ""
                    raw_link = urljoin(BASE_URL, link_tag["href"]) if link_tag and link_tag.get("href") else ""

                    if not raw_title:
                        continue

                    raw_text = f"{raw_title} {raw_status}".strip()
                    nlp = nlp_company_bonus(raw_text)

                    company = (nlp.get("company") or raw_title).strip()
                    bonus = nlp.get("bonus")

                    results.append(
                        {
                            "title": raw_title,
                            "link": raw_link,
                            "status": raw_status,
                            "company": company,
                            "bonus": bonus,
                        }
                    )
                except Exception as e:
                    print(f"    ‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ –∫–∞—Ä—Ç–æ—á–∫–∏ #{i}: {e}")

            return results, soup

        except requests.exceptions.RequestException as e:
            last_error = f"–û—à–∏–±–∫–∞ —Å–µ—Ç–∏/—Ç–∞–π–º–∞—É—Ç –Ω–∞ –ø–æ–ø—ã—Ç–∫–µ {attempt}: {e}"
            print(f"  ‚ö†Ô∏è {last_error}")
            time.sleep(2)
        except Exception as e:
            last_error = f"–û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞: {e}"
            print(f"  ‚ùå {last_error}")
            break

    print(f"  ‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å —Å—Ç—Ä–∞–Ω–∏—Ü—É –ø–æ—Å–ª–µ {retry_count} –ø–æ–ø—ã—Ç–æ–∫: {last_error}")
    return [], BeautifulSoup("", "lxml")


def _get_next_page_url(soup: BeautifulSoup) -> Optional[str]:
    """
    –ù–∞—Ö–æ–¥–∏—Ç URL —Å–ª–µ–¥—É—é—â–µ–π —Å—Ç—Ä–∞–Ω–∏—Ü—ã –∏–∑ –ø–∞–≥–∏–Ω–∞—Ü–∏–∏ –ë–µ–ª–∫–∞—Ä—Ç–∞.
    –û–ø–∏—Ä–∞–µ—Ç—Å—è –Ω–∞ –∞–∫—Ç–∏–≤–Ω—ã–π —ç–ª–µ–º–µ–Ω—Ç –∏ –Ω–æ–º–µ—Ä —Å–ª–µ–¥—É—é—â–µ–π —Å—Ç—Ä–∞–Ω–∏—Ü—ã.
    """
    active_page = soup.select_one("a.pagination-link.active")
    if not active_page:
        print("    ‚ÑπÔ∏è –ê–∫—Ç–∏–≤–Ω–∞—è —Å—Ç—Ä–∞–Ω–∏—Ü–∞ –ø–∞–≥–∏–Ω–∞—Ü–∏–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω–∞")
        return None

    try:
        current_num = int(active_page.text.strip())
    except (ValueError, AttributeError):
        print("    ‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –æ–ø—Ä–µ–¥–µ–ª–∏—Ç—å –Ω–æ–º–µ—Ä —Ç–µ–∫—É—â–µ–π —Å—Ç—Ä–∞–Ω–∏—Ü—ã")
        return None

    next_page_num = current_num + 1

    # 1) –ü—Ä—è–º–∞—è —Å—Å—ã–ª–∫–∞ –Ω–∞ PAGEN_1=next_page_num
    next_link = soup.select_one(f"a.pagination-link[href*='PAGEN_1={next_page_num}']")
    if next_link and next_link.get("href"):
        full_url = urljoin(BASE_URL, next_link["href"])
        print(f"    ‚úÖ –ù–∞–π–¥–µ–Ω–∞ —Å–ª–µ–¥—É—é—â–∞—è —Å—Ç—Ä–∞–Ω–∏—Ü–∞ ({next_page_num}): {full_url}")
        return full_url

    # 2) –ö–Ω–æ–ø–∫–∞ "–°–ª–µ–¥."
    next_btn = soup.select_one("a.pagination-button:not(.disabled)")
    if next_btn and next_btn.get("href") and next_btn["href"] != "javascript:void(0);":
        full_url = urljoin(BASE_URL, next_btn["href"])
        print(f"    ‚úÖ –ù–∞–π–¥–µ–Ω–∞ –∫–Ω–æ–ø–∫–∞ '–°–ª–µ–¥.': {full_url}")
        return full_url

    print(f"    ‚ÑπÔ∏è –°–ª–µ–¥—É—é—â–∞—è —Å—Ç—Ä–∞–Ω–∏—Ü–∞ ({next_page_num}) –Ω–µ –Ω–∞–π–¥–µ–Ω–∞ - —ç—Ç–æ –ø–æ—Å–ª–µ–¥–Ω—è—è —Å—Ç—Ä–∞–Ω–∏—Ü–∞")
    return None


# ---------- –î–ï–î–£–ü–õ–ò–ö–ê–¶–ò–Ø –ò –°–û–•–†–ê–ù–ï–ù–ò–ï ----------

def save_belkart_items(bank_id: int, items: List[Dict[str, Any]]) -> None:
    """
    –°–æ—Ö—Ä–∞–Ω—è–µ—Ç –ø–∞—Ä—Ç–Ω—ë—Ä–æ–≤ –ë–µ–ª–∫–∞—Ä—Ç–∞, –≥–∞—Ä–∞–Ω—Ç–∏—Ä—É—è:
    1. –í—Å–µ –≤ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ 0.
    2. –ù–µ—Ç –¥—É–±–ª–∏–∫–∞—Ç–æ–≤ –ø–æ –∫–æ–º–ø–∞–Ω–∏–∏.
    3. –í—ã–±–∏—Ä–∞–µ—Ç—Å—è –∑–∞–ø–∏—Å—å —Å –º–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–º —á–∏—Å–ª–æ–≤—ã–º –±–æ–Ω—É—Å–æ–º.
    4. –°—Å—ã–ª–∫–∏ —Å—Ç–∞—Ä–∞–µ–º—Å—è –Ω–µ —Ç–µ—Ä—è—Ç—å.
    """
    if not items:
        print("‚ö†Ô∏è –ù–µ—Ç –ø—Ä–µ–¥–º–µ—Ç–æ–≤ –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è")
        return

    grouped: Dict[str, List[Dict[str, Any]]] = defaultdict(list)

    # –ì—Ä—É–ø–ø–∏—Ä–æ–≤–∫–∞ –ø–æ –∫–æ–º–ø–∞–Ω–∏–∏
    for item in items:
        company = (item.get("company") or item.get("title") or "").strip()
        if not company:
            print(f"‚ö†Ô∏è –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –ø–∞—Ä—Ç–Ω—ë—Ä–∞ –±–µ–∑ –Ω–∞–∑–≤–∞–Ω–∏—è: {item}")
            continue

        bonus = normalize_bonus(item.get("bonus"))
        link = item.get("link") or ""

        grouped[company].append({"bonus": bonus, "link": link})

    partners_data: List[Dict[str, Any]] = []

    for company, records in grouped.items():
        if not records:
            print(f"‚ö†Ô∏è –ù–µ—Ç –∑–∞–ø–∏—Å–µ–π –¥–ª—è {company}")
            continue

        best_record: Optional[Dict[str, Any]] = None
        best_bonus_value = -1.0

        for rec in records:
            bonus = rec["bonus"]
            link = rec["link"]

            bonus_num = extract_bonus_number(bonus) if bonus else -1.0

            if (
                best_record is None
                or bonus_num > best_bonus_value
                or (bonus_num == best_bonus_value and link and not best_record.get("link"))
            ):
                best_bonus_value = bonus_num
                best_record = rec

        if best_record is None:
            continue

        final_link = best_record.get("link") or ""
        if not final_link:
            for rec in records:
                if rec.get("link"):
                    final_link = rec["link"]
                    break

        final_bonus = best_record.get("bonus")

        partners_data.append(
            {
                "partner_name": company,
                "partner_bonus": final_bonus,
                "partner_link": final_link,
            }
        )

        print(f"  ‚úÖ {company} ‚Üí –±–æ–Ω—É—Å: {final_bonus or '–Ω–µ—Ç'}, —Å—Å—ã–ª–∫–∞: {'–¥–∞' if final_link else '–Ω–µ—Ç'}")

    print(f"\nüìù –°–æ—Ö—Ä–∞–Ω—è—é {len(partners_data)} —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –ø–∞—Ä—Ç–Ω—ë—Ä–æ–≤...")
    save_partners(
        partners=partners_data,
        bank_id=bank_id,
        category_id=0,
    )
    print(f"‚úÖ –°–æ—Ö—Ä–∞–Ω–µ–Ω–æ {len(partners_data)} —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –ø–∞—Ä—Ç–Ω—ë—Ä–æ–≤ –ë–µ–ª–∫–∞—Ä—Ç–∞")


# ---------- –ì–õ–ê–í–ù–ê–Ø –§–£–ù–ö–¶–ò–Ø ----------

def fetch_promotions(
    bank_id: int,
    progress: ProgressFn = None,
    banks_done: int = 0,
    banks_total: int = 0,
) -> List[Dict[str, Any]]:
    """
    –ó–∞–≥—Ä—É–∂–∞–µ—Ç –í–°–ï —Å—Ç—Ä–∞–Ω–∏—Ü—ã –ë–µ–ª–∫–∞—Ä—Ç–∞ –∏ —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç –ø–∞—Ä—Ç–Ω—ë—Ä–æ–≤.
    –î–µ–ª–∞–µ—Ç:
    1. –ü—Ä–∞–≤–∏–ª—å–Ω—É—é –ø–∞–≥–∏–Ω–∞—Ü–∏—é.
    2. –ü–æ–≤—Ç–æ—Ä–Ω—ã–µ –ø–æ–ø—ã—Ç–∫–∏ –ø—Ä–∏ –æ—à–∏–±–∫–∞—Ö.
    3. –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –Ω–∞–∑–≤–∞–Ω–∏–π –∏ –±–æ–Ω—É—Å–æ–≤ —á–µ—Ä–µ–∑ GigaChat.
    4. –î–µ–¥—É–ø–ª–∏–∫–∞—Ü–∏—é –ø–æ –∫–æ–º–ø–∞–Ω–∏–∏.
    5. –ó–∞—â–∏—Ç—É –æ—Ç —Ü–∏–∫–ª–æ–≤ –ø–æ URL.
    """
    all_items: List[Dict[str, Any]] = []
    current_url = BASE_URL
    page_num = 1
    max_pages = 100
    visited_urls: set[str] = set()

    while page_num <= max_pages:
        note = f"[bank {bank_id}] üìÑ –ë–µ–ª–∫–∞—Ä—Ç ‚Äì —Å—Ç—Ä–∞–Ω–∏—Ü–∞ {page_num}"
        print(note)
        if progress:
            progress(banks_done, banks_total, note)

        if current_url in visited_urls:
            print(f"[bank {bank_id}] ‚ö†Ô∏è –¶–∏–∫–ª! –°—Ç—Ä–∞–Ω–∏—Ü–∞ —É–∂–µ –±—ã–ª–∞ –ø–æ—Å–µ—â–µ–Ω–∞: {current_url}")
            break
        visited_urls.add(current_url)

        items, soup = _parse_page(current_url)
        if not items:
            print(f"[bank {bank_id}] ‚ÑπÔ∏è –°—Ç—Ä–∞–Ω–∏—Ü–∞ {page_num} –ø—É—Å—Ç–∞ - –∫–æ–Ω–µ—Ü –∫–∞—Ç–∞–ª–æ–≥–∞")
            break

        all_items.extend(items)
        print(f"[bank {bank_id}] ‚úÖ –°—Ç—Ä. {page_num}: +{len(items)} –ø–∞—Ä—Ç–Ω—ë—Ä–æ–≤ (–≤—Å–µ–≥–æ: {len(all_items)})")

        next_url = _get_next_page_url(soup)
        if not next_url:
            print(f"[bank {bank_id}] ‚úÖ –î–æ—Å—Ç–∏–≥–Ω—É—Ç–∞ –ø–æ—Å–ª–µ–¥–Ω—è—è —Å—Ç—Ä–∞–Ω–∏—Ü–∞ ({page_num})")
            break

        current_url = next_url
        page_num += 1
        time.sleep(1)

    if all_items:
        print(f"\n[bank {bank_id}] üìä –í—Å–µ–≥–æ –∑–∞–≥—Ä—É–∂–µ–Ω–æ: {len(all_items)} –ø–∞—Ä—Ç–Ω—ë—Ä–æ–≤ —Å–æ —Å—Ç—Ä–∞–Ω–∏—Ü 1-{page_num}")
        save_belkart_items(bank_id, all_items)
    else:
        print(f"[bank {bank_id}] ‚ö†Ô∏è –ü–∞—Ä—Ç–Ω—ë—Ä—ã –Ω–µ –∑–∞–≥—Ä—É–∂–µ–Ω—ã")

    done_msg = f"[bank {bank_id}] ‚úÖ –ë–µ–ª–∫–∞—Ä—Ç –∑–∞–≤–µ—Ä—à—ë–Ω: {len(all_items)} –ø–∞—Ä—Ç–Ω—ë—Ä–æ–≤ –∑–∞–≥—Ä—É–∂–µ–Ω–æ"
    print(done_msg)
    if progress:
        progress(banks_done, banks_total, done_msg)

    return all_items
