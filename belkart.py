# belkart.py
import requests
import time
from bs4 import BeautifulSoup
from urllib.parse import urljoin, urlparse, parse_qs
from dotenv import load_dotenv
from collections import defaultdict
from typing import List, Dict, Any, Optional, Callable, Tuple

from db_sql import save_single_category, save_partners_with_status_logic

ProgressFn = Optional[Callable[[int, int, str], None]]

BASE_URL = "https://belkart.by/BELKART/reklamnye-aktsii/"

from gigachat import GigaChat
import json
import os

load_dotenv()

GIGACHAT_TOKEN = os.getenv("GIGACHAT_TOKEN")

gc = GigaChat(
    credentials=GIGACHAT_TOKEN,
    scope="GIGACHAT_API_B2B",
    verify_ssl_certs=False,
    model="GigaChat-2-Max",
)
print("GIGACHAT_TOKEN =", GIGACHAT_TOKEN)

_GIGA_CACHE: Dict[str, Dict[str, Any]] = {}


def _parse_page(url: str, retry_count: int = 3) -> Tuple[List[Dict[str, Any]], BeautifulSoup]:
    """
    –ü–∞—Ä—Å–∏—Ç —Å—Ç—Ä–∞–Ω–∏—Ü—É —Å –ø–æ–≤—Ç–æ—Ä–Ω—ã–º–∏ –ø–æ–ø—ã—Ç–∫–∞–º–∏ –ø—Ä–∏ –æ—à–∏–±–∫–∞—Ö.
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ø–∏—Å–æ–∫ –ø–∞—Ä—Ç–Ω—ë—Ä–æ–≤ –∏ –æ–±—ä–µ–∫—Ç BeautifulSoup.
    """
    last_error = None
    
    for attempt in range(retry_count):
        try:
            print(f"  üì° –ü–æ–ø—ã—Ç–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ {attempt+1}/{retry_count}: {url}")
            
            resp = requests.get(url, timeout=20)
            resp.raise_for_status()
            soup = BeautifulSoup(resp.text, "lxml")

            cards = soup.select("ul.card-list li.card-list__item")
            results = []
            
            print(f"    üîç –ù–∞–π–¥–µ–Ω–æ –∫–∞—Ä—Ç–æ—á–µ–∫: {len(cards)}")

            for i, card in enumerate(cards):
                try:
                    link_tag = card.select_one("a.card-list__link")
                    title_tag = card.select_one(".card-list__title")
                    status_tag = card.select_one(".card-list__label")

                    raw_title = title_tag.text.strip() if title_tag else ""
                    raw_status = status_tag.text.strip() if status_tag else ""
                    raw_link = urljoin(BASE_URL, link_tag["href"]) if link_tag and link_tag.get("href") else ""
                    
                    if not raw_title:
                        continue
                    
                    # –û–±—ä–µ–¥–∏–Ω—è–µ–º —Ç–µ–∫—Å—Ç –¥–ª—è GigaChat
                    raw_text = f"{raw_title} {raw_status}".strip()
                    
                    # –ö—ç—à –∏–ª–∏ GigaChat
                    if raw_text in _GIGA_CACHE:
                        nlp = _GIGA_CACHE[raw_text]
                    else:
                        nlp = extract_company_and_bonus(raw_text)
                        _GIGA_CACHE[raw_text] = nlp

                    company = nlp.get("company") or raw_title
                    bonus = nlp.get("bonus")
                    
                    company = company.strip()
                    
                    results.append({
                        "title": raw_title,
                        "link": raw_link,
                        "status": raw_status,
                        "company": company,
                        "bonus": bonus,
                    })

                except Exception as e:
                    print(f"    ‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ –∫–∞—Ä—Ç–æ—á–∫–∏ #{i+1}: {e}")
                    continue

            return results, soup
            
        except requests.exceptions.Timeout:
            last_error = f"–¢–∞–π–º–∞—É—Ç –Ω–∞ –ø–æ–ø—ã—Ç–∫–µ {attempt+1}"
            print(f"  ‚è±Ô∏è {last_error}, –ø–æ–≤—Ç–æ—Ä—è–µ–º...")
            time.sleep(2)
            
        except requests.exceptions.RequestException as e:
            last_error = f"–û—à–∏–±–∫–∞ —Å–µ—Ç–∏: {e}"
            print(f"  ‚ö†Ô∏è {last_error}, –ø–æ–≤—Ç–æ—Ä—è–µ–º...")
            time.sleep(2)
            
        except Exception as e:
            last_error = f"–û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞: {e}"
            print(f"  ‚ùå {last_error}")
            return [], BeautifulSoup("", "lxml")
    
    print(f"  ‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å —Å—Ç—Ä–∞–Ω–∏—Ü—É –ø–æ—Å–ª–µ {retry_count} –ø–æ–ø—ã—Ç–æ–∫: {last_error}")
    return [], BeautifulSoup("", "lxml")


def _get_next_page_url(soup: BeautifulSoup, current_page: int) -> Optional[str]:
    """
    –ù–∞—Ö–æ–¥–∏—Ç URL —Å–ª–µ–¥—É—é—â–µ–π —Å—Ç—Ä–∞–Ω–∏—Ü—ã –∏–∑ –ø–∞–≥–∏–Ω–∞—Ü–∏–∏ –ë–µ–ª–∫–∞—Ä—Ç–∞.
    
    –°—Ç—Ä—É–∫—Ç—É—Ä–∞:
    <div class="pagination">
        <ul class="pagination-list">
            <li><a class="pagination-link active" href="javascript:void(0);">1</a></li>
            <li><a class="pagination-link" href="/BELKART/reklamnye-aktsii/?PAGEN_1=2">2</a></li>
            <li><a class="pagination-button" href="/BELKART/reklamnye-aktsii/?PAGEN_1=2">
    """
    # –ò—â–µ–º –∞–∫—Ç–∏–≤–Ω—É—é —Å—Ç—Ä–∞–Ω–∏—Ü—É
    active_page = soup.select_one("a.pagination-link.active")
    if not active_page:
        print(f"    ‚ÑπÔ∏è –ê–∫—Ç–∏–≤–Ω–∞—è —Å—Ç—Ä–∞–Ω–∏—Ü–∞ –ø–∞–≥–∏–Ω–∞—Ü–∏–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω–∞")
        return None
    
    # –ü–æ–ª—É—á–∞–µ–º —Ç–µ–∫—Å—Ç –∞–∫—Ç–∏–≤–Ω–æ–π —Å—Ç—Ä–∞–Ω–∏—Ü—ã (–Ω–æ–º–µ—Ä)
    try:
        current_num = int(active_page.text.strip())
    except (ValueError, AttributeError):
        print(f"    ‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –æ–ø—Ä–µ–¥–µ–ª–∏—Ç—å –Ω–æ–º–µ—Ä —Ç–µ–∫—É—â–µ–π —Å—Ç—Ä–∞–Ω–∏—Ü—ã")
        return None
    
    # –ò—â–µ–º —Å—Å—ã–ª–∫—É –Ω–∞ —Å–ª–µ–¥—É—é—â—É—é —Å—Ç—Ä–∞–Ω–∏—Ü—É
    next_page_num = current_num + 1
    
    # –í–∞—Ä–∏–∞–Ω—Ç 1: –ü—Ä—è–º–∞—è —Å—Å—ã–ª–∫–∞ –Ω–∞ —Å–ª–µ–¥—É—é—â–∏–π –Ω–æ–º–µ—Ä –≤ –ø–∞–≥–∏–Ω–∞—Ü–∏–∏
    next_link = soup.select_one(f"a.pagination-link[href*='PAGEN_1={next_page_num}']")
    if next_link and next_link.get("href"):
        full_url = urljoin(BASE_URL, next_link.get("href"))
        print(f"    ‚úÖ –ù–∞–π–¥–µ–Ω–∞ —Å–ª–µ–¥—É—é—â–∞—è —Å—Ç—Ä–∞–Ω–∏—Ü–∞ ({next_page_num}): {full_url}")
        return full_url
    
    # –í–∞—Ä–∏–∞–Ω—Ç 2: –ö–Ω–æ–ø–∫–∞ "–°–ª–µ–¥." (Next button)
    next_btn = soup.select_one("a.pagination-button:not(.disabled)")
    if next_btn and next_btn.get("href") and next_btn.get("href") != "javascript:void(0);":
        full_url = urljoin(BASE_URL, next_btn.get("href"))
        print(f"    ‚úÖ –ù–∞–π–¥–µ–Ω–∞ –∫–Ω–æ–ø–∫–∞ '–°–ª–µ–¥.': {full_url}")
        return full_url
    
    # –í–∞—Ä–∏–∞–Ω—Ç 3: –ü—Ä–æ–≤–µ—Ä—è–µ–º –≤—Å–µ —Å—Å—ã–ª–∫–∏ –ø–∞–≥–∏–Ω–∞—Ü–∏–∏
    all_links = soup.select("a.pagination-link")
    link_numbers = []
    for link in all_links:
        try:
            num = int(link.text.strip())
            link_numbers.append((num, link))
        except (ValueError, AttributeError):
            continue
    
    if link_numbers:
        link_numbers.sort(key=lambda x: x[0])
        for num, link in link_numbers:
            if num == next_page_num and link.get("href"):
                full_url = urljoin(BASE_URL, link.get("href"))
                print(f"    ‚úÖ –ù–∞–π–¥–µ–Ω–∞ —Å—Å—ã–ª–∫–∞ –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü—É {next_page_num}: {full_url}")
                return full_url
    
    print(f"    ‚ÑπÔ∏è –°–ª–µ–¥—É—é—â–∞—è —Å—Ç—Ä–∞–Ω–∏—Ü–∞ ({next_page_num}) –Ω–µ –Ω–∞–π–¥–µ–Ω–∞ - —ç—Ç–æ –ø–æ—Å–ª–µ–¥–Ω—è—è —Å—Ç—Ä–∞–Ω–∏—Ü–∞")
    return None


def extract_company_and_bonus(text: str) -> dict:
    """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –∫–æ–º–ø–∞–Ω–∏—é –∏ –±–æ–Ω—É—Å —á–µ—Ä–µ–∑ GigaChat"""
    if not text.strip():
        return {"company": None, "bonus": None}

    prompt = f"""
        –ò–∑–≤–ª–µ–∫–∏ –∏–∑ —Ç–µ–∫—Å—Ç–∞ –Ω–∞–∑–≤–∞–Ω–∏–µ –∫–æ–º–ø–∞–Ω–∏–∏ –∏ —Ä–∞–∑–º–µ—Ä –±–æ–Ω—É—Å–∞.
        –ï—Å–ª–∏ –±–æ–Ω—É—Å —É–∫–∞–∑–∞–Ω –≤ —Ä–∞–∑–Ω—ã—Ö —Ñ–æ—Ä–º–∞—Ç–∞—Ö (—Å–∫–∏–¥–∫–∞ 15%, 15%, -15%, –∫–µ—à–±—ç–∫ 20%, 2 –∫–Ω–∏–≥–∏ –≤ –ø–æ–¥–∞—Ä–æ–∫), –≤–µ—Ä–Ω–∏ –µ–≥–æ –∫–∞–∫ –µ—Å—Ç—å.

        –¢–µ–∫—Å—Ç:
        \"\"\"{text}\"\"\"

        –í–µ—Ä–Ω–∏ –°–¢–†–û–ì–û JSON –±–µ–∑ –ø–æ—è—Å–Ω–µ–Ω–∏–π:
        {{
        "company": "...",
        "bonus": "..."
        }}

        –ï—Å–ª–∏ –¥–∞–Ω–Ω—ã—Ö –Ω–µ—Ç ‚Äì –∏—Å–ø–æ–ª—å–∑—É–π null.
        """

    try:
        resp = gc.chat(prompt)
        raw = resp.choices[0].message.content.strip()

        raw = raw.replace("```json", "").replace("```", "").strip()
        data = json.loads(raw)

        company = data.get("company")
        bonus = data.get("bonus")
        
        if bonus:
            bonus = str(bonus).strip()
            bonus = " ".join(bonus.split())

        return {
            "company": company,
            "bonus": bonus if bonus else None,
        }

    except Exception as e:
        print(f"‚ö†Ô∏è GigaChat error: {e}")
        return {
            "company": None,
            "bonus": None,
        }


def save_belkart_items(bank_id: int, items: list):
    """
    –°–æ—Ö—Ä–∞–Ω—è–µ—Ç –ø–∞—Ä—Ç–Ω—ë—Ä–æ–≤ –ë–µ–ª–∫–∞—Ä—Ç–∞, –≥–∞—Ä–∞–Ω—Ç–∏—Ä—É—è:
    1. –í—Å–µ –≤ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ 0
    2. –ù–µ—Ç –¥—É–±–ª–∏–∫–∞—Ç–æ–≤ –ø–æ –∫–æ–º–ø–∞–Ω–∏–∏
    3. –ú–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–π/–ª—É—á—à–∏–π –±–æ–Ω—É—Å –¥–ª—è –∫–∞–∂–¥–æ–π –∫–æ–º–ø–∞–Ω–∏–∏
    4. –°—Å—ã–ª–∫–∏ –≤—Å–µ–≥–¥–∞ –∑–∞–ø–æ–ª–Ω–µ–Ω—ã
    """
    if not items:
        print("‚ö†Ô∏è –ù–µ—Ç –ø—Ä–µ–¥–º–µ—Ç–æ–≤ –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è")
        return

    # –ì—Ä—É–ø–ø–∏—Ä—É–µ–º –ø–æ –∫–æ–º–ø–∞–Ω–∏–∏: –∫–æ–º–ø–∞–Ω–∏—è ‚Üí —Å–ø–∏—Å–æ–∫ –∑–∞–ø–∏—Å–µ–π (–±–æ–Ω—É—Å, —Å—Å—ã–ª–∫–∞)
    grouped = defaultdict(list)
    
    for item in items:
        company = (item.get("company") or item.get("title") or "").strip()
        
        if not company:
            print(f"‚ö†Ô∏è –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –ø–∞—Ä—Ç–Ω—ë—Ä–∞ –±–µ–∑ –Ω–∞–∑–≤–∞–Ω–∏—è: {item}")
            continue
        
        bonus = item.get("bonus")
        link = item.get("link") or ""
        
        # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º –±–æ–Ω—É—Å
        if bonus:
            bonus = str(bonus).strip()
            bonus = " ".join(bonus.split())
        
        grouped[company].append({
            "bonus": bonus,
            "link": link,
        })

    # –§–æ—Ä–º–∞—Ç–∏—Ä—É–µ–º —Ñ–∏–Ω–∞–ª—å–Ω—ã–π —Å–ø–∏—Å–æ–∫ –±–µ–∑ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤
    partners_data = []
    
    for company, records in grouped.items():
        best_record = records[0] if records else None  
        best_bonus_value = -1
        
        if not best_record:
            print(f"‚ö†Ô∏è –ù–µ—Ç –∑–∞–ø–∏—Å–µ–π –¥–ª—è {company}")
            continue
        
        # –í—ã–±–∏—Ä–∞–µ–º –∑–∞–ø–∏—Å—å —Å –º–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–º —á–∏—Å–ª–æ–≤—ã–º –±–æ–Ω—É—Å–æ–º
        for rec in records:
            bonus = rec.get("bonus")
            link = rec.get("link")
            
            # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º –±–æ–Ω—É—Å –µ—Å–ª–∏ —ç—Ç–æ —Å–ø–∏—Å–æ–∫
            if isinstance(bonus, list):
                bonus = " ".join(str(b).strip() for b in bonus if b)
                rec["bonus"] = bonus if bonus else None
            
            # –ò–∑–≤–ª–µ–∫–∞–µ–º —á–∏—Å–ª–æ–≤–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ –±–æ–Ω—É—Å–∞
            bonus_num = extract_bonus_number(bonus) if bonus else -1
            
            # –í—ã–±–∏—Ä–∞–µ–º –ª—É—á—à–∏–π –±–æ–Ω—É—Å
            if bonus_num > best_bonus_value or (bonus_num == best_bonus_value and link and not best_record.get("link")):
                best_bonus_value = bonus_num
                best_record = rec
        
        # –ï—Å–ª–∏ –Ω–µ—Ç —Å—Å—ã–ª–∫–∏, –∏—â–µ–º —Å—Ä–µ–¥–∏ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤
        final_link = best_record.get("link") or ""
        if not final_link:
            for rec in records:
                if rec.get("link"):
                    final_link = rec.get("link")
                    break

        final_bonus = best_record.get("bonus")
        
        # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º —Ñ–∏–Ω–∞–ª—å–Ω—ã–π –±–æ–Ω—É—Å –µ—Å–ª–∏ —ç—Ç–æ —Å–ø–∏—Å–æ–∫
        if isinstance(final_bonus, list):
            final_bonus = " ".join(str(b).strip() for b in final_bonus if b) or None

        partners_data.append({
            "partner_name": company,
            "partner_bonus": final_bonus,
            "partner_link": final_link
        })
        
        print(f"  ‚úÖ {company} ‚Üí –±–æ–Ω—É—Å: {final_bonus or '–Ω–µ—Ç'}, —Å—Å—ã–ª–∫–∞: {'–¥–∞' if final_link else '–Ω–µ—Ç'}")

    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ –ë–î
    print(f"\nüìù –°–æ—Ö—Ä–∞–Ω—è—é {len(partners_data)} —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –ø–∞—Ä—Ç–Ω—ë—Ä–æ–≤...")
    save_partners_with_status_logic(
        partners=partners_data,
        bank_id=bank_id,
        category_id=0
    )
    
    print(f"‚úÖ –°–æ—Ö—Ä–∞–Ω–µ–Ω–æ {len(partners_data)} —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –ø–∞—Ä—Ç–Ω—ë—Ä–æ–≤ –ë–µ–ª–∫–∞—Ä—Ç–∞")


def extract_bonus_number(bonus_str: Optional[str]) -> float:
    """
    –ò–∑–≤–ª–µ–∫–∞–µ—Ç —á–∏—Å–ª–æ–≤–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ –∏–∑ —Å—Ç—Ä–æ–∫–∏ –±–æ–Ω—É—Å–∞.
    –ü—Ä–∏–º–µ—Ä—ã: "—Å–∫–∏–¥–∫–∞ 15%" ‚Üí 15, "20 –¥–Ω–µ–π" ‚Üí 20, "2 –∫–Ω–∏–≥–∏ –≤ –ø–æ–¥–∞—Ä–æ–∫" ‚Üí 2
    """
    if not bonus_str:
        return -1
    
    import re
    
    # –ò—â–µ–º —á–∏—Å–ª–∞ –≤ —Å—Ç—Ä–æ–∫–µ
    numbers = re.findall(r'\d+(?:[\.,]\d+)?', str(bonus_str))
    
    if numbers:
        try:
            # –ë–µ—Ä—ë–º –ø–µ—Ä–≤–æ–µ —á–∏—Å–ª–æ (–æ–±—ã—á–Ω–æ —Å–∞–º–æ–µ –∑–Ω–∞—á–∏–º–æ–µ)
            return float(numbers[0].replace(',', '.'))
        except:
            return -1
    
    return -1


def fetch_promotions(
    bank_id: int,
    progress: ProgressFn = None,
    banks_done: int = 0,
    banks_total: int = 0,
) -> List[Dict[str, Any]]:
    """
    –ó–∞–≥—Ä—É–∂–∞–µ—Ç –í–°–ï —Å—Ç—Ä–∞–Ω–∏—Ü—ã –ë–µ–ª–∫–∞—Ä—Ç–∞ –∏ —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç –ø–∞—Ä—Ç–Ω—ë—Ä–æ–≤.
    –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç:
    1. –í—Å–µ —Å—Ç—Ä–∞–Ω–∏—Ü—ã —Å –ø—Ä–∞–≤–∏–ª—å–Ω–æ–π –ø–∞–≥–∏–Ω–∞—Ü–∏–µ–π
    2. –ü–æ–≤—Ç–æ—Ä–Ω—ã–µ –ø–æ–ø—ã—Ç–∫–∏ –ø—Ä–∏ –æ—à–∏–±–∫–∞—Ö
    3. –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –Ω–∞–∑–≤–∞–Ω–∏–π –∏ –±–æ–Ω—É—Å–æ–≤ —á–µ—Ä–µ–∑ GigaChat
    4. –î–µ–¥—É–ø–ª–∏–∫–∞—Ü–∏—é –ø–æ –∫–æ–º–ø–∞–Ω–∏–∏
    5. –ó–∞—â–∏—Ç–∞ –æ—Ç —Ü–∏–∫–ª–æ–≤ - –æ—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏–µ –ø–æ—Å–µ—â—ë–Ω–Ω—ã—Ö URL
    """
    all_items: List[Dict[str, Any]] = []
    current_url = BASE_URL
    page_num = 1
    max_pages = 100  # –ó–∞—â–∏—Ç–∞ –æ—Ç –±–µ—Å–∫–æ–Ω–µ—á–Ω–æ–≥–æ —Ü–∏–∫–ª–∞
    visited_urls = set()  # –û—Ç—Å–ª–µ–∂–∏–≤–∞–µ–º –ø–æ—Å–µ—â—ë–Ω–Ω—ã–µ —Å—Ç—Ä–∞–Ω–∏—Ü—ã

    while page_num <= max_pages:
        note = f"[bank {bank_id}] üìÑ –ë–µ–ª–∫–∞—Ä—Ç ‚Äì —Å—Ç—Ä–∞–Ω–∏—Ü–∞ {page_num}"
        print(note)
        if progress:
            progress(banks_done, banks_total, note)

        # ‚úÖ –ü—Ä–æ–≤–µ—Ä—è–µ–º, –Ω–µ –ø–æ—Å–µ—â–∞–ª–∏ –ª–∏ —É–∂–µ —ç—Ç—É —Å—Ç—Ä–∞–Ω–∏—Ü—É (–∑–∞—â–∏—Ç–∞ –æ—Ç —Ü–∏–∫–ª–æ–≤)
        if current_url in visited_urls:
            print(f"[bank {bank_id}] ‚ö†Ô∏è –û–±–Ω–∞—Ä—É–∂–µ–Ω —Ü–∏–∫–ª! –°—Ç—Ä–∞–Ω–∏—Ü–∞ —É–∂–µ –±—ã–ª–∞ –ø–æ—Å–µ—â–µ–Ω–∞: {current_url}")
            print(f"[bank {bank_id}] ‚úÖ –ó–∞–≤–µ—Ä—à—ë–Ω –ø–∞—Ä—Å–∏–Ω–≥ - –≤—Å–µ–≥–æ –∑–∞–≥—Ä—É–∂–µ–Ω–æ —Å—Ç—Ä–∞–Ω–∏—Ü: {len(visited_urls)}")
            break
        
        visited_urls.add(current_url)

        try:
            items, soup = _parse_page(current_url)
        except Exception as e:
            err = f"[bank {bank_id}] ‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ —Å—Ç—Ä–∞–Ω–∏—Ü—ã {page_num}: {e}"
            print(err)
            if progress:
                progress(banks_done, banks_total, err)
            break

        if not items:
            print(f"[bank {bank_id}] ‚ÑπÔ∏è –°—Ç—Ä–∞–Ω–∏—Ü–∞ {page_num} –ø—É—Å—Ç–∞ - –∫–æ–Ω–µ—Ü –∫–∞—Ç–∞–ª–æ–≥–∞")
            break

        all_items.extend(items)
        print(f"[bank {bank_id}] ‚úÖ –°—Ç—Ä–∞–Ω–∏—Ü–∞ {page_num}: +{len(items)} –ø–∞—Ä—Ç–Ω—ë—Ä–æ–≤ (–≤—Å–µ–≥–æ: {len(all_items)})")
        
        # –ò—â–µ–º —Å–ª–µ–¥—É—é—â—É—é —Å—Ç—Ä–∞–Ω–∏—Ü—É
        next_url = _get_next_page_url(soup, page_num)
        
        if not next_url:
            print(f"[bank {bank_id}] ‚úÖ –î–æ—Å—Ç–∏–≥–Ω—É—Ç–∞ –ø–æ—Å–ª–µ–¥–Ω—è—è —Å—Ç—Ä–∞–Ω–∏—Ü–∞ ({page_num})")
            break

        current_url = next_url
        page_num += 1
        time.sleep(1)  # –ó–∞–¥–µ—Ä–∂–∫–∞ –º–µ–∂–¥—É –∑–∞–ø—Ä–æ—Å–∞–º–∏

    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤—Å–µ –ø–∞—Ä—Ç–Ω—ë—Ä—ã –æ–¥–Ω–∏–º –∑–∞–ø—Ä–æ—Å–æ–º
    if all_items:
        print(f"\n[bank {bank_id}] üìä –í—Å–µ–≥–æ –∑–∞–≥—Ä—É–∂–µ–Ω–æ: {len(all_items)} –ø–∞—Ä—Ç–Ω—ë—Ä–æ–≤ —Å–æ —Å—Ç—Ä–∞–Ω–∏—Ü 1-{page_num-1}")
        save_belkart_items(bank_id, all_items)
    else:
        print(f"[bank {bank_id}] ‚ö†Ô∏è –ü–∞—Ä—Ç–Ω—ë—Ä—ã –Ω–µ –∑–∞–≥—Ä—É–∂–µ–Ω—ã")
    
    done = f"[bank {bank_id}] ‚úÖ –ë–µ–ª–∫–∞—Ä—Ç –∑–∞–≤–µ—Ä—à—ë–Ω: {len(all_items)} –ø–∞—Ä—Ç–Ω—ë—Ä–æ–≤ –∑–∞–≥—Ä—É–∂–µ–Ω–æ"
    print(done)
    if progress:
        progress(banks_done, banks_total, done)

    return all_items